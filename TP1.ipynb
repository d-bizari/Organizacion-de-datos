{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importacion general de librerias y de visualizacion (matplotlib y seaborn)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib\n",
    "#plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "sns.set(style=\"whitegrid\") # seteando tipo de grid en seaborn\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.groupby('location').agg({'target':['count','sum']})[('target','sum')].nlargest(10) #Top ten ubicaciones con mas tweets verdaderos"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ahora nos interesaria investigar que porcentaje de verdad tiene cada location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grouped_location = raw_data.groupby('location').agg({'target':['mean','count','sum']})\n",
    "data_grouped_location.columns = ['target_mean','target_count','target_sum']\n",
    "data_grouped_location.sort_values(by='target_sum',ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Podemos ver a simple vista que hay lugares que no tienen mucha informaci√≥n, por lo que vamos a realizar un filtrado para obtener los lugares que tengan al menos 10 muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grouped_location = data_grouped_location.drop(data_grouped_location[data_grouped_location['target_count'] < 10].index)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ahora que nuestro set de datos tiene ubicaciones con la suficiente informacion muestral podemos calcular el porcentaje de veracidad por location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=data_grouped_location['target_mean'].nlargest(10) * 100,y=data_grouped_location['target_mean'].nlargest(10).index,orient='h')\n",
    "ax.set_title(\"Top 10 truth percentege per location\", fontsize = 15)\n",
    "ax.set_xlabel(\"Location\", fontsize = 12)\n",
    "ax.set_ylabel(\"Truth percentege\", fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_most_tweeted = data_grouped_location['target_count'].nlargest(10).index\n",
    "index_most_tweeted = index_most_tweeted.drop('unknown') #Ya que en la limpieza de datos determinamos que son valores desconocidos\n",
    "f, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x=data_grouped_location.loc[index_most_tweeted,'target_count'], y=index_most_tweeted,\n",
    "            label=\"All tweets\", color=\"b\")\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x=data_grouped_location.loc[index_most_tweeted,'target_sum'], y=index_most_tweeted,\n",
    "            label=\"Truth tweets\", color=\"b\")\n",
    "\n",
    "# Add a legend and informative axis label\n",
    "ax.set_title(\"Top 10 most tweeted location\", fontsize = 15)\n",
    "ax.set_ylabel(\"Location\", fontsize = 12)\n",
    "# ax.set_xlabel(\"Truth percentege\", fontsize = 12)\n",
    "ax.legend(ncol=2, loc=\"lower right\", frameon=True)\n",
    "ax.set(ylabel=\"Location\",xlabel=\"Total\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no pareciera haber relacion entre la cantidad de veces mencionada con su verdad\n",
    "data_grouped_location = data_grouped_location.drop(data_grouped_location.loc[data_grouped_location.index == 'unknown'].index)\n",
    "sns.scatterplot(x=data_grouped_location['target_count'],y=data_grouped_location['target_sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribucion de locations verdaderas vs distribucion de location falsas\n",
    "g = sns.distplot(data_grouped_location['target_sum'],bins=50,hist=True) #tweets verdaderos por location\n",
    "g = sns.distplot(data_grouped_location['target_count'],bins=50,hist=True) #tweets por location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the data for location-keyword relationshp analysis\n",
    "twitterKeywordAndLocation = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv',\\\n",
    "                                        usecols = ['keyword', 'location'])\n",
    "twitterKeywordAndLocation = twitterKeywordAndLocation[twitterKeywordAndLocation['keyword'] != 'unknown']\n",
    "twitterKeywordAndLocation = twitterKeywordAndLocation[twitterKeywordAndLocation['location'] != 'unknown']\n",
    "twitterKeywordAndLocation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some of DataFrame's properties\n",
    "twitterKeywordAndLocation.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterKeywordAndLocation.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing DataFrmae columns data types in order to apply some operations on them \n",
    "\n",
    "twitterKeywordAndLocation['keyword'] = twitterKeywordAndLocation['keyword'].astype('string')\n",
    "twitterKeywordAndLocation['location'] = twitterKeywordAndLocation['location'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the data\n",
    "\n",
    "#Dropping null values\n",
    "twitterKeywordAndLocation.dropna(inplace = True)\n",
    "twitterKeywordAndLocation.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We consider that those locations containing non-alphanumerical characters are not real\n",
    "#Dropping false locations\n",
    "twitterKeywordAndLocation = twitterKeywordAndLocation[~twitterKeywordAndLocation['location'].str.isalnum()]\n",
    "twitterKeywordAndLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping those locations that don't have a significant amount of keywords\n",
    "#For that, first we see the average\n",
    "twitterKeywordAndLocation['location'].value_counts().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then, we filter\n",
    "twitterKeywordAndLocation = twitterKeywordAndLocation.groupby('location').filter(lambda x: len(x) > 4)\n",
    "twitterKeywordAndLocation.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "\n",
    "#Counter of keywords\n",
    "twitterKeywordAndLocation['counter'] = 1\n",
    "twitterKeywordAndLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amount of keywords per location\n",
    "keywordsPerLocation = twitterKeywordAndLocation.groupby('location').agg({'counter' : 'sum'}).sort_values(by = 'counter', ascending = False)\n",
    "keywordsPerLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot\n",
    "\n",
    "ax = keywordsPerLocation.head(20).plot(kind = 'bar', figsize = (18, 8), rot = 45,\\\n",
    "                                   title = 'Amount of keywords per location',\\\n",
    "                                  color = 'purple')\n",
    "ax.set_ylabel('Keyword counter', size = 14)\n",
    "ax.set_xlabel('Location', size = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most popular keywords\n",
    "keywordsPopular = twitterKeywordAndLocation.groupby('keyword').agg({'counter' : 'sum'}).sort_values(by = 'counter', ascending = False)\n",
    "keywordsPopular.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordsPopular = keywordsPopular[keywordsPopular['counter'] > 1]\n",
    "keywordsPopular.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot\n",
    "\n",
    "ax = keywordsPopular.head(20).plot(kind = 'bar', figsize = (18, 8), rot = 45,\\\n",
    "                                   title = 'Most popular keywords',\\\n",
    "                                  color = 'green')\n",
    "ax.set_ylabel('Amount', size = 14)\n",
    "ax.set_xlabel('Keyword', size = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison between locations with most keywords and most popular keywords\n",
    "locationAndKeyword = twitterKeywordAndLocation\n",
    "locationAndKeyword = pd.merge(locationAndKeyword, keywordsPopular, on = 'keyword')\n",
    "locationAndKeyword['counter'] = locationAndKeyword['counter_x'] + locationAndKeyword['counter_y']\n",
    "locationAndKeyword = locationAndKeyword.drop(['counter_x', 'counter_y'], axis = 1)\n",
    "locationAndKeyword.fillna(0)\n",
    "locationAndKeyword = locationAndKeyword.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplot\n",
    "\n",
    "g = sns.relplot(x = 'keyword', y = 'location', hue = 'keyword', size = 'counter',\\\n",
    "            sizes = (40, 400), alpha = .5, height = 8, data = locationAndKeyword)\n",
    "g.ax.set_title('Keywords per location', fontsize = 20)\n",
    "g.set_xlabels('Keyword',fontsize = 15)\n",
    "g.set_ylabels('Location', fontsize = 15)\n",
    "g.ax.figure.set_size_inches(28, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsInfo = pd.read_csv('./train.csv', encoding = 'latin-1', usecols = ['text', 'target'])\n",
    "tweetsInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting the analisis for the relation between keywords and hashtags\n",
    "hashForKeywordsAndHashtags = {}\n",
    "csvFormatted = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv', usecols = ['keyword', 'text', 'target'])\n",
    "csvFormatted = csvFormatted[csvFormatted['keyword'] != 'unknown']\n",
    "csvFormatted['keyword'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumHashtagIfNedeed(line, keyword, hashOfKeywords):\n",
    "    for word in line.split():\n",
    "        if not word.startswith('#'):\n",
    "            continue\n",
    "        word = word.lower().lstrip('#')\n",
    "        if keyword not in hashOfKeywords:\n",
    "            hashOfKeywords[keyword] = {}\n",
    "        hashOfKeywords[keyword][word.lstrip('#')] = hashOfKeywords[keyword].get(word.lstrip('#'), 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in csvFormatted.iterrows():\n",
    "    sumHashtagIfNedeed(row['text'], row['keyword'], hashForKeywordsAndHashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'keyword': [], 'amount': []}\n",
    "for key in hashForKeywordsAndHashtags:\n",
    "    d['keyword'].append(key)\n",
    "    d['amount'].append(sum(hashForKeywordsAndHashtags[key].values()))\n",
    "keywordDf = pd.DataFrame(d, columns =['keyword', 'amount'])\n",
    "keywordDf = keywordDf[keywordDf['amount'] > 15].sort_values(by = ['amount'])\n",
    "keywordDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x = 'keyword', y = 'amount', data = keywordDf);\n",
    "ax.set_title('Keyword and amount of hashtags', fontsize=20, color = 'red')\n",
    "ax.set_xlabel('Keywords', fontsize = 18, color = 'red')\n",
    "ax.set_ylabel('Hashtags used', fontsize = 18, color ='red')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.tick_params(axis=\"x\", labelsize='large')\n",
    "ax.tick_params(axis=\"x\", labelsize=16)\n",
    "ax.tick_params(axis=\"y\", labelsize=16)\n",
    "ax.figure.set_size_inches(20, 8);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvWithOnlyKeywordTarget = csvFormatted.drop('text', 1)\n",
    "csvWithOnlyKeywordTarget = csvWithOnlyKeywordTarget.groupby(['keyword']).agg({'target': ['mean', 'count']})\n",
    "csvWithOnlyKeywordTarget.columns = csvWithOnlyKeywordTarget.columns.get_level_values(0) + '_' + csvWithOnlyKeywordTarget.columns.get_level_values(1)\n",
    "csvWithOnlyKeywordTarget = csvWithOnlyKeywordTarget.sort_values(by = ['target_mean']).reset_index() #Hasta aca tengo TODOS los valores de verdad\n",
    "csvWithOnlyKeywordTarget = pd.merge(csvWithOnlyKeywordTarget, keywordDf, on='keyword', how='inner')\n",
    "csvWithOnlyKeywordTarget.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x = 'keyword', y = 'target_mean', data = csvWithOnlyKeywordTarget);\n",
    "ax.set_title('Keyword and veracity value', fontsize=20, color = 'red')\n",
    "ax.set_xlabel('Keywords', fontsize = 18, color = 'red')\n",
    "ax.set_ylabel('Veracity', fontsize = 18, color ='red')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.tick_params(axis=\"x\", labelsize='large')\n",
    "ax.tick_params(axis=\"x\", labelsize=16)\n",
    "ax.tick_params(axis=\"y\", labelsize=16)\n",
    "ax.figure.set_size_inches(20, 8);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsInfo['tweet_length'] = tweetsInfo.text.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validUser(userName):\n",
    "    if '@' in userName:\n",
    "        user = getter(userName, '@')\n",
    "        length = len(user)\n",
    "        if (length > 1 and length <= 16):\n",
    "            for char in user[1:]:\n",
    "                if not(char.isalnum() or char == '_'): return False\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validLink(link):\n",
    "    type1 = 'https://'\n",
    "    type2 = 'http://'\n",
    "    if type1 in link and len(link) > 9: return True\n",
    "    if type2 in link and len(link) > 8: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validHashtag(hashtag):\n",
    "    if '#' in hashtag:\n",
    "        hashtag = getter(hashtag, '#')\n",
    "        hashtag = hashtag[1:]\n",
    "        return hashtag.isalnum()\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to analyze the elements (#. @, links) of the tweet\n",
    "def analyzeTweets(text):\n",
    "    result = [0,0,0] #number of usersTagged, hashtags and links\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        if validUser(word): result[0] += 1\n",
    "        elif validHashtag(word): result[1] += 1\n",
    "        elif validLink(word): result[2] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates a new DF, char = # or @\n",
    "#dicc is a dictionary, key: @user or #hashtag, value: [number of occurrence, number of true targets]\n",
    "#func1 get the hashtag or user correctly\n",
    "#func2 cheks if the result of func1 is correct\n",
    "#text its a combination of two columns, text and target, the target is in the last position always\n",
    "def dataFrameMaker(text, dicc, char, func1, func2):\n",
    "    text = text.split()\n",
    "    target = int(text[-1])\n",
    "    for word in text:\n",
    "        if char in word:\n",
    "            auxString = func1(word, char)  #auxString could be a @user or a #hashtag\n",
    "            if func2(auxString):\n",
    "                auxString = auxString.lower()\n",
    "                auxList = dicc[auxString] = dicc.get(auxString, [0,0])\n",
    "                auxList[0] += 1\n",
    "                auxList[1] += target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Col1: column of the DF to filter\n",
    "#condition: condition to filter\n",
    "#col2: must be of type str\n",
    "#col3: its type will be transform into str\n",
    "#return a Serie with the combination of col2 and col3\n",
    "def colsCombination(col1, condition, col2, col3):\n",
    "        filterCondition = tweetsInfo[col1] > condition\n",
    "        strCol2 = tweetsInfo[filterCondition][col2]\n",
    "        strCol3 = tweetsInfo[filterCondition][col3]\n",
    "        strCol3 = strCol3.astype(str)\n",
    "        result = strCol2 + ' ' + strCol3\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the hashtag or user\n",
    "def getter(text, char):\n",
    "    pos = text.find(char)\n",
    "    text = text[pos:]\n",
    "    #Some users or hashtags finish with : or .\n",
    "    if text.endswith(':') or text.endswith('.'):\n",
    "        text = text[:-1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Col1: column of the DF to filter\n",
    "#condition: condition to filter\n",
    "#col2: must be of type str\n",
    "#col3: its type will be transform into str\n",
    "#return a Serie with the combination of col2 and col3\n",
    "def colsCombination(col1, condition, col2, col3):\n",
    "        filterCondition = tweetsInfo[col1] > condition\n",
    "        strCol2 = tweetsInfo[filterCondition][col2]\n",
    "        strCol3 = tweetsInfo[filterCondition][col3]\n",
    "        strCol3 = strCol3.astype(str)\n",
    "        result = strCol2 + ' ' + strCol3\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfo(dataList, pos):\n",
    "    return dataList[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aux column to get the result\n",
    "tweetsInfo['aux_column'] = tweetsInfo.text.apply(analyzeTweets)\n",
    "\n",
    "tweetsInfo['users_tagged'] = tweetsInfo.aux_column.apply(getInfo,args=(0,))\n",
    "tweetsInfo['hashtags'] = tweetsInfo.aux_column.apply(getInfo,args=(1,))\n",
    "tweetsInfo['links'] = tweetsInfo.aux_column.apply(getInfo,args=(2,))\n",
    "\n",
    "del tweetsInfo['aux_column']\n",
    "tweetsInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the usersTagged df\n",
    "usersDicc = {}\n",
    "tweetsInfoTags = colsCombination('users_tagged',0,'text','target')\n",
    "tweetsInfoTags.apply(dataFrameMaker, args = (usersDicc,'@',getter,validUser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersSerie = pd.Series(usersDicc)\n",
    "usersDataFrame = usersSerie.to_frame(name='auxCol')\n",
    "usersDataFrame['occurrence'] = usersDataFrame.auxCol.apply(getInfo,args=(0,))\n",
    "usersDataFrame['target_sum'] = usersDataFrame.auxCol.apply(getInfo,args=(1,))\n",
    "del usersDataFrame['auxCol']\n",
    "usersDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most mentioned users\n",
    "topMentions = usersDataFrame[usersDataFrame.occurrence > 5].occurrence.nlargest(10)\n",
    "topMentions.sort_values(ascending=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 mentioned users barplot\n",
    "colors = {'@youtube': 'red','@arianagrande':'plum', '@justinbieber':'violet',\\\n",
    "         '@potus':'limegreen', '@change':'yellow', '@usatoday':'steelblue',\\\n",
    "         '@foxnews':'c', '@emmerdale':'silver', '@djicemoon':'skyblue', '@mikeparractor':'lightsalmon'}\n",
    "ax = sns.barplot(x = topMentions.index, y = topMentions, palette = colors);\n",
    "ax.set_title('Top 10: Mentioned users', fontsize=20)\n",
    "ax.set_xlabel('Users', fontsize = 18)\n",
    "ax.set_ylabel('Total mentions', fontsize = 18)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.tick_params(axis=\"both\", labelsize=16)\n",
    "ax.figure.set_size_inches(14, 8);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the hashtags df\n",
    "hashtagsDicc = {}\n",
    "tweetsInfoHashtags = colsCombination('hashtags',0,'text','target')\n",
    "tweetsInfoHashtags.apply(dataFrameMaker, args = (hashtagsDicc,'#',getter,validHashtag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsSerie = pd.Series(hashtagsDicc)\n",
    "hashtagsDataFrame = hashtagsSerie.to_frame(name='auxCol')\n",
    "hashtagsDataFrame['occurrence'] = hashtagsDataFrame.auxCol.apply(getInfo, args=(0,))\n",
    "hashtagsDataFrame['target_sum'] = hashtagsDataFrame.auxCol.apply(getInfo, args=(1,))\n",
    "del hashtagsDataFrame['auxCol']\n",
    "hashtagsDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trending topics barplot\n",
    "trendingTopics = hashtagsDataFrame[hashtagsDataFrame.occurrence > 10].occurrence.nlargest(10)\n",
    "trendingTopics.sort_values(ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x = trendingTopics.index, y = trendingTopics);\n",
    "ax.set_title('Trending topics', fontsize=20)\n",
    "ax.set_xlabel('Hashtags', fontsize = 18)\n",
    "ax.set_ylabel('Total mentions', fontsize = 18)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.tick_params(axis=\"both\", labelsize=16)\n",
    "ax.figure.set_size_inches(14, 8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = tweetsInfo.groupby('tweet_length').agg({'target':'sum','text':'count','hashtags':'sum','users_tagged':'sum','links':'sum'})\n",
    "grouped['total_elements'] = grouped.links + grouped.hashtags + grouped.users_tagged\n",
    "grouped['truth_percentage'] = (grouped.target / grouped.text) * 100\n",
    "grouped.index.rename('lengths', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_quantity, min_quantity = grouped.text.max(), grouped.text.min()\n",
    "max_quantity, min_quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.drop(grouped[grouped.text <= 10].index, inplace=True)\n",
    "grouped.reset_index(inplace = True)\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regplot 1\n",
    "ax = sns.regplot(x='lengths', y='truth_percentage', data=grouped,\\\n",
    "                line_kws = {'color':'lightsalmon','alpha':0.5,'lw':3},\\\n",
    "                color = 'brown')\n",
    "\n",
    "ax.set_xlabel('Tweet length(characters)', fontsize = 14)\n",
    "ax.set_ylabel('Veracity (%)', fontsize = 14)\n",
    "ax.set_yticks(np.arange(0,110,10))\n",
    "ax.set_title('Tweet length vs veracity', fontsize=16)\n",
    "ax.figure.set_size_inches(14,4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot\n",
    "ax = sns.scatterplot(x=\"lengths\", y=\"truth_percentage\", size=\"total_elements\",  data=grouped, sizes = (30,300))\n",
    "ax.set_title('PONER TITULO', fontsize = 18)\n",
    "ax.set_xlabel('Tweet length(characters)', fontsize = 14)\n",
    "ax.set_ylabel('Veracity (%)', fontsize = 14)\n",
    "ax.set_yticks(np.arange(0,110,10))\n",
    "ax.figure.set_size_inches(14, 8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural disasters\n",
    "df1 = pd.read_csv('./ToChangeKeywordsAndLocations/formattedTests.csv', usecols=['keyword','location'])\n",
    "df2 = tweetsInfo\n",
    "disastersDF = pd.concat([df1,df2], axis = 1)\n",
    "disastersDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some auxiliar functions\n",
    "def getSeriesElements(serie, setElements):\n",
    "    for element in serie.values: #Element is a string always\n",
    "        if '/' in element:\n",
    "            element = element.split('/')\n",
    "            for elemt in element: setElements.add(elemt.lower())\n",
    "                \n",
    "        else: setElements.add(element.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the info is between position 2 and 6, both included\n",
    "def obtainInfo(infoList):\n",
    "    naturalDisasters = {} #Key: group, value: {subgroups}\n",
    "    for i in range (2,7): #To iterate the info in the list\n",
    "        dataFrame = infoList[i]\n",
    "        #Always delete the first row, it dosent have info\n",
    "        dataFrame.drop(0, inplace = True)\n",
    "        #The group always is at (0,1)\n",
    "        group = dataFrame.iloc[0,1]\n",
    "        #Now its time to iterate the columns of the DF\n",
    "        cols = len(dataFrame.columns)\n",
    "        subgroups = set()\n",
    "        for col in range(2, cols):\n",
    "            serie = dataFrame[col] #This is a serie\n",
    "            serie.dropna(inplace=True)\n",
    "            serie.drop_duplicates(inplace=True)\n",
    "            getSeriesElements(serie, subgroups)\n",
    "        naturalDisasters[group] = subgroups\n",
    "    return naturalDisasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the info about natural disasters\n",
    "#naturalDisastersDicc key: group value: set of subgroups\n",
    "dataPage = pd.read_html('https://www.emdat.be/classification')\n",
    "naturalDisastersDicc = obtainInfo(dataPage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding missing items\n",
    "geo = naturalDisastersDicc['Geophysical']\n",
    "geo.update({'volcano', 'sinkhole', 'lava'})\n",
    "\n",
    "met = naturalDisastersDicc['Meteorological']\n",
    "met.update({'hurricane','typhoon','twister','cyclone','hailstorm',\\\n",
    "            'violent storm','rainstorm','sandstorm','snowstorm','windstorm'})\n",
    "met -= {'lightning','derecho','sand','wind'}\n",
    "\n",
    "hydro = naturalDisastersDicc['Hydrological']\n",
    "hydro.update({'debris','mudslide','avalanche','rockfall'})\n",
    "hydro.remove('avalanche (snow, debris, mudflow, rockfall)')\n",
    "\n",
    "clima = naturalDisastersDicc['Climatological']\n",
    "clima.update({'bush fire', 'land fire', 'brush fire'})\n",
    "clima.remove('land fire: brush, bush,  pasture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new set with the union of all the subgroups\n",
    "allNaturalDisasters = set()\n",
    "for value in naturalDisastersDicc.values():\n",
    "    allNaturalDisasters = allNaturalDisasters.union(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some keywords are about natural disasters but they are in plural\n",
    "#we fix that whit this function\n",
    "def fixingKeywords(keyword):\n",
    "    auxDictionary = {'floods':'flood', 'wild fires': 'wildfire', 'forest fires':'forest fire',\\\n",
    "                    'bush fires':'bush fire'}\n",
    "    return auxDictionary.get(keyword, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disastersDF.keyword = disastersDF.keyword.apply(fixingKeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = ~(disastersDF.keyword.isin(allNaturalDisasters))\n",
    "naturalDisastersDF = disastersDF.drop(disastersDF[condition].index)\n",
    "naturalDisastersDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by subgroup of natural disaster\n",
    "natDisastGrouped = naturalDisastersDF.groupby('keyword').agg({'tweet_length':['max','min','mean'],\\\n",
    "                                                             'text':'count','target':'sum',\\\n",
    "                                                             'users_tagged':'sum','hashtags':'sum','links':'sum'})\n",
    "natDisastGrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the labels\n",
    "labels0 = natDisastGrouped.columns.get_level_values(0)\n",
    "labels1 = natDisastGrouped.columns.get_level_values(1)\n",
    "natDisastGrouped.columns = labels0 + '_' + labels1\n",
    "natDisastGrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to obtain the group of a keyword\n",
    "def naturalDisasterGroup(keyword):\n",
    "    for key, value in naturalDisastersDicc.items():\n",
    "        if keyword in value: return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natDisastGrouped.reset_index(inplace=True)\n",
    "#Adding the column 'group', to the data frama\n",
    "natDisastGrouped['group'] = natDisastGrouped.keyword.apply(naturalDisasterGroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natDisastGrouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natDisastGrouped.rename(columns = {'keyword':'subgroup'},inplace=True)\n",
    "natDisastGrouped.sort_values(by='group',inplace=True)  #easy to order, has 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natDisastGrouped.set_index(['group','subgroup'],inplace=True)\n",
    "natDisastGrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the column 'truth_percentage' = (target_sum / text_count) * 100\n",
    "natDisastGrouped['truth_percentage'] = (natDisastGrouped.target_sum / natDisastGrouped.text_count) * 100\n",
    "natDisastGrouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veracity of the subgroups\n",
    "subVeracity = natDisastGrouped.reset_index().sort_values(by='truth_percentage',ascending=False)\n",
    "ax = sns.barplot(x = 'truth_percentage', y = subVeracity.subgroup, data = subVeracity);\n",
    "ax.set_title('Natural disasters subgroups: veracity', fontsize=20)\n",
    "ax.set_xlabel('veracity(%)', fontsize = 18)\n",
    "ax.set_ylabel('Natural Disasters subgroups', fontsize = 18)\n",
    "ax.tick_params(axis=\"x\", labelsize='large')\n",
    "ax.tick_params(axis=\"x\", labelsize=16)\n",
    "ax.tick_params(axis=\"y\", labelsize=16)\n",
    "ax.set_xticks(np.arange(0,110,10))\n",
    "ax.figure.set_size_inches(10, 8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parallel coordinates\n",
    "from pandas.plotting import parallel_coordinates\n",
    "df = natDisastGrouped.reset_index().sort_values(by='truth_percentage',ascending=False)[:5]\n",
    "lineColors = ('firebrick','cadetblue','orange','forestgreen','goldenrod')\n",
    "\n",
    "ax = parallel_coordinates(df, 'subgroup', cols = ['truth_percentage', 'text_count','target_sum','links_sum', 'users_tagged_sum','hashtags_sum'],color = lineColors, lw = 5.0)\n",
    "ax.set_title('Top5 subgroups: characteristics', fontsize= 16)\n",
    "ax.figure.set_size_inches(16, 8)\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis of kind of hashtags used in tweets based on tweet's veracity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsInfo = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv', usecols = ['text', 'target'])\n",
    "tweetsInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Receives a text\n",
    "#Returns a list containing all valid hashtags on the text\n",
    "#A hashtag is valid if it only contains alphanumeric values\n",
    "def getValidHashtags(text, char):\n",
    "    resultingHashtags = []\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        hashtag = getter(word, char)\n",
    "        if validHashtag(hashtag) == True:\n",
    "                resultingHashtags.append(hashtag)\n",
    "    return resultingHashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsInfo = pd.read_csv('./ToChangeKeywordsAndLocations/withoutEncoding.csv', usecols = ['text', 'target'])\n",
    "tweetsInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsDataFrame = hashtagsDataFrame.reset_index()\n",
    "hashtagsDataFrame = hashtagsDataFrame.rename(columns = {'index' : 'hashtag'})\n",
    "hashtagsDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textPerVeracity = tweetsInfo.groupby('target').agg({'text' : 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a DF with hashtags included in tweets of veracity 'target', their occurrence and target_sum\n",
    "#target = 0 -> false tweets\n",
    "#target = 1 -> real tweets\n",
    "def hashtagPerVeracityDFMaker(target, char):\n",
    "    df = pd.DataFrame()\n",
    "    df['hashtag'] = getValidHashtags(textPerVeracity.loc[target,'text'], char)\n",
    "    df = hashtagsDataFrame.merge(df, on = 'hashtag')\n",
    "    df = df.drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating DF with hashtags and the veracity of the tweets containing them\n",
    "#Hashtags in false tweets:\n",
    "DFHashtagPerFalseTweets = hashtagPerVeracityDFMaker(0, '#')\n",
    "DFHashtagPerFalseTweets['occurrence'] = DFHashtagPerFalseTweets['occurrence'] - DFHashtagPerFalseTweets['target_sum']\n",
    "del DFHashtagPerFalseTweets['target_sum']\n",
    "DFHashtagPerFalseTweets = DFHashtagPerFalseTweets.sort_values(by = 'occurrence',\\\n",
    "                                                             ascending = False)\n",
    "DFHashtagPerFalseTweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot\n",
    "ax = sns.barplot(x = 'hashtag', y = 'occurrence', data = DFHashtagPerFalseTweets.head(35),\\\n",
    "                 palette = \"husl\")\n",
    "ax.set_xlabel('Hashtags', fontsize = 15)\n",
    "ax.set_ylabel('Occurrences', fontsize = 15)\n",
    "ax.set_title('Hashtags in false tweets', fontsize = 20)\n",
    "plt.xticks(rotation=65, horizontalalignment='right')\n",
    "ax.figure.set_size_inches(15, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hashtags in real tweets:\n",
    "DFHashtagPerRealTweets = hashtagPerVeracityDFMaker(1, '#')\n",
    "DFHashtagPerRealTweets['occurrence'] = DFHashtagPerRealTweets['target_sum']\n",
    "del DFHashtagPerRealTweets['target_sum']\n",
    "DFHashtagPerRealTweets = DFHashtagPerRealTweets.sort_values(by = 'occurrence',\\\n",
    "                                                           ascending = False)\n",
    "DFHashtagPerRealTweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot\n",
    "ax = sns.barplot(x = 'hashtag', y = 'occurrence', data = DFHashtagPerRealTweets.head(35),\\\n",
    "                 palette = \"husl\")\n",
    "ax.set_xlabel('Hashtags', fontsize = 15)\n",
    "ax.set_ylabel('Occurrences', fontsize = 15)\n",
    "ax.set_title('Hashtags in real tweets', fontsize = 20)\n",
    "plt.xticks(rotation=65, horizontalalignment='right')\n",
    "ax.figure.set_size_inches(15, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison between hashtags that appear both in real and false tweets\n",
    "hashtagsPerVeracity = DFHashtagPerFalseTweets.merge(DFHashtagPerRealTweets, on = 'hashtag')\n",
    "hashtagsPerVeracity.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagsPerVeracity['total occurrence'] = hashtagsPerVeracity['occurrence_x'] + hashtagsPerVeracity['occurrence_y']\n",
    "hashtagsPerVeracity = hashtagsPerVeracity.rename(columns = {'occurrence_y' : 'occurrence real tweets'})\n",
    "del hashtagsPerVeracity['occurrence_x']\n",
    "hashtagsPerVeracity = hashtagsPerVeracity.sort_values(by = 'total occurrence',\\\n",
    "                                                     ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barplot\n",
    "f, ax = plt.subplots(figsize = (15, 8))\n",
    "sns.set_color_codes('dark')\n",
    "sns.barplot(x = 'total occurrence', y = 'hashtag', data = hashtagsPerVeracity.head(20),\\\n",
    "            label = 'Total hashtag occurrence', color = 'm', edgecolor = 'w')\n",
    "sns.set_color_codes('muted')\n",
    "sns.barplot(x = 'occurrence real tweets', y = 'hashtag', data = hashtagsPerVeracity.head(20),\n",
    "            label = 'Real tweets hashtag occurrence', color = 'g', edgecolor = 'w')\n",
    "ax.legend(ncol = 2, loc = 'lower right')\n",
    "ax.set_xlabel('Ocurrence', fontsize = 15)\n",
    "ax.set_ylabel('Hashtag', fontsize = 15)\n",
    "ax.set_title('Hashtags and their veracity', fontsize = 20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
