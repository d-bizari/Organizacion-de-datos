{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "keywordSet = set()\n",
    "citiesSet = set()\n",
    "countriesSet = set()\n",
    "locationsSet = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CorrectString(setWithPossibleKeywords, text, considerHashtag):\n",
    "    word = 'Unknown'\n",
    "    for each in setWithPossibleKeywords:\n",
    "        if (each in text):\n",
    "            word = each;\n",
    "            break\n",
    "    if (considerHashtag and word == 'Unknown'):\n",
    "        for each in text.split():\n",
    "            if (each.startswith('#')):\n",
    "                word = each.lstrip('#')\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixMissingFields(setWithPossibleKeywords, dataframe, nameOfColumnWithParamMissing = 'keyword', nameOfPlaceWithText = 'text', useHashtag = True): \n",
    "    for index, row in dataframe.iterrows():\n",
    "        if (isinstance(row[nameOfColumnWithParamMissing], str)):\n",
    "            continue\n",
    "        dataframe.loc[index, nameOfColumnWithParamMissing] = CorrectString(setWithPossibleKeywords, row[nameOfPlaceWithText], useHashtag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToSet(nameOfSet, dataFrameRows):\n",
    "    for each in dataFrameRows:\n",
    "        nameOfSet.add(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the sets and texts\n",
    "tweets = pd.read_csv('train.csv', encoding = 'latin-1')\n",
    "dfWithKeywords = tweets.dropna(subset=['keyword'])\n",
    "dfWithLocations = tweets.dropna(subset=['location'])\n",
    "locations = pd.read_csv('worldcities.csv', encoding = 'latin-1')\n",
    "addToSet(keywordSet, dfWithKeywords['keyword'])\n",
    "addToSet(locationsSet, dfWithLocations['location'])\n",
    "addToSet(citiesSet, locations['city'])\n",
    "addToSet(countriesSet, locations['country'])\n",
    "arrayWithSets = [locationsSet, citiesSet, countriesSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix missing fields\n",
    "fixMissingKeywords(keywordSet, tweets)\n",
    "for eachSet in arrayWithSets:\n",
    "    fixMissingKeywords(eachSet, tweets, 'location', 'text', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tallahassee Florida               1\n",
       "louisville, kentucky              1\n",
       "London, Sydney                    1\n",
       "ATLANTA , GEORGIA                 1\n",
       "instagram: bribriony              1\n",
       "whs '17                           1\n",
       "Akure city in ondo state          1\n",
       "Halifax, Nova Scotia              1\n",
       "Spokane, Washington 99206         1\n",
       "North Highlands, CA               1\n",
       "In the moment                     1\n",
       "eARth 3                           1\n",
       "Southern Califorina               1\n",
       "Elsewhere, NZ                     1\n",
       "sitting on Eddie Vedders lap,     1\n",
       "somewhere in cali                 1\n",
       "beijing .China                    1\n",
       "Wolmers Trust School for Boys     1\n",
       "Manchester, The World, England    1\n",
       "Alliston Ontario                  1\n",
       "IG : Sincerely_TSUNAMI            1\n",
       "highlands&slands scotland         1\n",
       "Garrett                           1\n",
       "Pontevedra, Galicia               1\n",
       "Albuquerque                       1\n",
       "Lima, Peru                        1\n",
       "Long Island, NY                   1\n",
       "they/her                          1\n",
       "Pro-American and Anti-#Occupy     1\n",
       "Pelham, AL                        1\n",
       "emily | helen | shelley           1\n",
       "Antioch, CA                       1\n",
       "Canadian bread                    1\n",
       "austin tx                         1\n",
       "Wherever I'm needed               1\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['location'].value_counts().head(35)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
